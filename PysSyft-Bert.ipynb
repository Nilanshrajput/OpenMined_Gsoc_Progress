{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "PysSyft-Bert.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/OpenMined_Gsoc_Progress/blob/master/PysSyft-Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX1c-q96IUZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "c8bb66fa-5742-4600-912a-3d0282fa25bc"
      },
      "source": [
        "#!pip install 'syft[udacity]'\n",
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 10.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=c809640996673bdf6922a54d7a705dc4e4ae6f4819b0d163d83871baf35f1427\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4seOJIBZIUJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy\n",
        "import torch\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcqIbOZ8Ivoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a couple workers\n",
        "\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE7SLTRGL2KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir(\"bert-base-uncased\")\n",
        "os.mkdir(\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AWfyU7-JRth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/19018/1053282/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587571018&Signature=fwhs4MKzQldqTtkIVjlMdSwoT6yVTiYkE3EFSielSEnF46sJGKrKiwdLIYP%2F1axzJ5SDAgeY5rR8evqoWI0jRFO1i%2FdhTRrAq2lU0RYaz10p4HRN%2BgVfanD7RnBs0ck4hFBP9ySvwWTrmPn9wOkn%2FUt%2FjerP87cMa4j0YeT5iWjDNdZ03yUT0FfyHnhB3pGBy%2FFZEi4Q0Lv0LFU6biS%2FRp07tjZvAROl1POuZlYmLEq%2BUK7N8HUC21nlFCrLy2iAsKKbrPOUZBfJuh7ohIJrW2%2BONKtwwJ1hJc6frTqL6etqvyct0FBsgL3m9dm6pFTYQ7BGUDx42ZioQG2gj8ljPw%3D%3D&response-content-disposition=attachment%3B+filename%3Djigsaw-multilingual-toxic-comment-classification.zip\" -c -O 'jigsaw-multilingual-toxic-comment-classification.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn8B2J5UKeo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/jigsaw-multilingual-toxic-comment-classification.zip -d /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6yBDOf5KxhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/431504/819665/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587571188&Signature=mYI15idqV4d0omPx5rIicdhZOIJ%2BDqUzafPHGQ2uXfC4k0TrA73htXO7zEYBIrdj%2Fkhq65Hq%2Bk%2Fx4n%2FZfz2l%2Fa00xXG3r6aaQu2oDBZ4C51dfNK3ScPn2reVjeLFvkbHZY8dq8a4Ag4JODSY5CVKEqDRpsF65JhBRBGolDeIlc7nSTHy72uUqnrB%2BHWpHQjeIyDLMaIx5P8AXTBLQ3wJxKXKVvIDeX%2FtAfDlIXZVSt6lO8ipzBqnxnhZGzBu4ZGAw0agS04htarJSYGjwwYJ7mfHKSRCydEpFUrXZPsss3p3AW6nlG%2BFCyYUc4DXiD5RALFeKemGpMkNy3Ux9YG%2FSA%3D%3D&response-content-disposition=attachment%3B+filename%3Dbert-base-uncased.zip\" -c -O 'bert-base-uncased.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br6rLEAbKxYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fbea9b39-4a47-42d5-93d9-0567acb5574e"
      },
      "source": [
        "!unzip bert-base-uncased.zip -d /content/bert-base-uncased"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bert-base-uncased.zip\n",
            "  inflating: /content/bert-base-uncased/config.json  \n",
            "  inflating: /content/bert-base-uncased/pytorch_model.bin  \n",
            "  inflating: /content/bert-base-uncased/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESA0AYOkIK1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VY0wK8wIK1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 10\n",
        "BERT_PATH = \"/content/bert-base-uncased\"\n",
        "MODEL_PATH = \"model.bin\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "    BERT_PATH,\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtFUdME0IK11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDataset:\n",
        "    def __init__(self, comment_text, target):\n",
        "        self.comment_text = comment_text\n",
        "        self.target = target\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        comment_text = str(self.comment_text[item])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        padding_length = self.max_len - len(ids)\n",
        "        ids += ([0]*padding_length)\n",
        "        mask += ([0]*padding_length)\n",
        "        token_type_ids += ([0]*padding_length)\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.target[item], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTDIAMDuIK16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768*2, 1)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        o1, _ = self.bert(\n",
        "            ids, \n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        mean_pool = torch.mean(o1,1)\n",
        "        max_pool,_ = torch.max(o1,1)\n",
        "        cat = torch.cat((mean_pool, max_pool),1)\n",
        "\n",
        "        bo = self.bert_drop(cat)\n",
        "        output = self.out(bo)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMwh4Qg2IK1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "\n",
        "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets = d[\"targets\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            targets = d[\"targets\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxibiBRxIK2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "\n",
        "    df1 = pd.read_csv(\"jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"])\n",
        "    df2 = pd.read_csv(\"jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"])\n",
        "    \n",
        "    df_train = pd.concat([df1, df2], axis = 0).reset_index(drop = True)\n",
        "    \n",
        "    df_valid = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "    )\n",
        "\n",
        "    train_dataset = dataset.BERTDataset(\n",
        "        comment_text=df_train.comment_text.values,\n",
        "        target=df_train.toxic.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    valid_dataset = dataset.BERTDataset(\n",
        "        comment_text=df_valid.comment_text.values,\n",
        "        target=df_valid.toxic.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = BERTBaseUncased()\n",
        "    model.to(device)\n",
        "    \n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        engine.train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        outputs, targets = engine.eval_fn(valid_data_loader, model, device)\n",
        "        targets = np.array(targets) >= 0.5\n",
        "        accuracy = metrics.roc_auc_score(targets, outputs)\n",
        "        print(f\"Accuracy Score = {accuracy}\")\n",
        "        if accuracy > best_accuracy:\n",
        "            torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "            best_accuracy = accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW1r_D_WMfyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}